[BASIC]
# The optimizer used for training
# Adadelta | Adagrad | Adam | AdamW | SparseAdam | Adamax |
# ASGD | LBFGS | NAdam | RAdam | RMSprop | Rprop | SGD
Optimizer = SGD

# Initial learning rate
LearningRate = 0.01

# Momentum used in optimizer
Momentum = 0.9

# Weight decay
WeightDecay = 0.0001

# Weight decay for Normalization layers (float)
NormWeightDecay = None

# The maximum gradient norm (float)
ClipGradNorm = None

# Weight decay for bias of all layers (float)
BiasWeightDecay = None

# Label smoothing
LabelSmoothing = 0.0

[TRANSFORMERS]
# Weight decay for embedding parameters in vision transformer models
EmbeddingDecay = None

[LR_SCHEDULER]
# Scheduler type
# LambdaLR | MultiplicativeLR | StepLR | MultiStepLR |
# ConstantLR | LinearLR | ExponentialLR | PolynomialLR |
# CosineAnnealingLR | ChainedScheduler | SequentialLR |
# ReduceLROnPlateau | CyclicLR | OneCycleLR | CosineAnnealingWarmRestarts
Type = StepLR

# Warmup epochs
WarmupEpochs = 0

# Warmup method
# Linear | Constant
WarmupMethod = Constant

# Warmup decay
WarmupDecay = 0.01

# Step size - Decrease LR every N epochs
StepSize = 30

# Gamma - Factor by which the LR is decreased
Gamma = 0.1

# Minimum value that the LR can reach
Min = 0.0

[DISTRIBUTED]
# Number of distributed processes
WorldSize = 1

# URL for distributed training
Url = env://

# Enable tracking Exponential Moving Average of model parameters
EMA = True

# Number of iterations that controls how often to update the EMA model
EmaSteps = 32

# Decay factor for EMA of model parameters
EmaDecay = 0.99998

